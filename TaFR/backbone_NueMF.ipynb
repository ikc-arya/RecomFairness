{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.12.4 (main, Jun 16 2024, 01:58:47) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "Pandas version: 2.2.3\n",
      "Tensorflow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from datetime import datetime\n",
    "\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.models.ncf.ncf_singlenode import NCF\n",
    "from recommenders.models.ncf.dataset import Dataset as NCFDataset\n",
    "from recommenders.evaluation.python_evaluation import (\n",
    "    map, ndcg_at_k, precision_at_k, recall_at_k\n",
    ")\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(\"System version:\", sys.version)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Tensorflow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "SEED = 42\n",
    "TOP_K = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_df = pd.read_csv(\"data/MIND/MINDsmall_train/behaviors.tsv\", sep=\"\\t\",\n",
    "                           names=[\"impression_id\", \"user_id\", \"timestamp\", \"history\", \"impressions\"])\n",
    "\n",
    "news_df = pd.read_csv(\"data/MIND/MINDsmall_train/news.tsv\", sep=\"\\t\",\n",
    "                      names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"entity_list\", \"relation_list\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(interactions_df[\"timestamp\"].head())\n",
    "\n",
    "# print(interactions_df[\"timestamp\"].dtype)\n",
    "# print(interactions_df[\"timestamp\"].apply(type).value_counts())\n",
    "\n",
    "# print(pd.to_datetime(interactions_df[\"timestamp\"].head(100)))\n",
    "\n",
    "# print(interactions_df[\"timestamp\"].isna().sum())\n",
    "# print((interactions_df[\"timestamp\"].astype(str) == \"\").sum())\n",
    "\n",
    "# interactions_df[\"timestamp\"] = pd.to_datetime(interactions_df[\"timestamp\"], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:recommenders.models.ncf.dataset:Indexing data/MIND/mind_interactions_train.csv ...\n"
     ]
    }
   ],
   "source": [
    "# Process interactions\n",
    "def process_impressions(row):\n",
    "    impressions = row[\"impressions\"].split(\" \")\n",
    "    return [(row[\"user_id\"], imp.split(\"-\")[0], int(imp.split(\"-\")[1]), row[\"timestamp\"])\n",
    "            for imp in impressions]\n",
    "\n",
    "interactions = []\n",
    "for _, row in behaviors_df.iterrows():\n",
    "    interactions.extend(process_impressions(row))\n",
    "\n",
    "interactions_df = pd.DataFrame(interactions, columns=[\"userID\", \"itemID\", \"rating\", \"timestamp\"])\n",
    "\n",
    "# -*- the NCFDataet function reads userID in int only so removing the letter 'U' and 'N' from IDs -*-\n",
    "# Create mappings for user IDs and item IDs\n",
    "user_mapping = {user: idx for idx, user in enumerate(interactions_df[\"userID\"].unique())}\n",
    "item_mapping = {item: idx for idx, item in enumerate(interactions_df[\"itemID\"].unique())}\n",
    "\n",
    "# Apply mappings\n",
    "interactions_df[\"userID\"] = interactions_df[\"userID\"].map(user_mapping)\n",
    "interactions_df[\"itemID\"] = interactions_df[\"itemID\"].map(item_mapping)\n",
    "\n",
    "# Apply the same item mapping to news_df\n",
    "news_df[\"news_id\"] = news_df[\"news_id\"].map(item_mapping)\n",
    "\n",
    "# Filter interactions for only available news articles\n",
    "interactions_df = interactions_df[interactions_df[\"itemID\"].isin(news_df[\"news_id\"])]\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "interactions_df[\"timestamp\"] = pd.to_datetime(interactions_df[\"timestamp\"])\n",
    "\n",
    "# Sort interactions by userID before saving\n",
    "interactions_df.sort_values(by=\"userID\", inplace=True)\n",
    "\n",
    "# First, group interactions to get the minimum timestamp per itemID\n",
    "item_min_timestamps = interactions_df.groupby(\"itemID\")[\"timestamp\"].min().reset_index()\n",
    "item_min_timestamps.rename(columns={\"itemID\": \"news_id\", \"timestamp\": \"upload_time\"}, inplace=True)\n",
    "\n",
    "# Merge with news_df\n",
    "news_df = news_df.merge(item_min_timestamps, on=\"news_id\", how=\"left\")\n",
    "news_df.rename(columns={\"upload_time_x\": \"upload_time\"}, inplace=True)\n",
    "# news_df[\"upload_time\"] = news_df[\"upload_time_y\"].fillna(news_df[\"upload_time_x\"])\n",
    "# news_df.drop(columns=[\"upload_time_x\", \"upload_time_y\"], inplace=True)\n",
    "\n",
    "news_df.dropna(subset=[\"upload_time\"], inplace=True)\n",
    "news_df.sort_values(by=\"upload_time\", inplace=True)\n",
    "\n",
    "# Mark 20% latest uploaded news as \"new items\"\n",
    "n_new_items = int(0.2 * len(news_df))\n",
    "new_items = set(news_df.tail(n_new_items)[\"news_id\"].values)\n",
    "\n",
    "\n",
    "# Save all processed interactions\n",
    "interactions_df.to_csv(\"data/MIND/mind_interactions_train.csv\", index=False)\n",
    "\n",
    "train_file = \"data/MIND/mind_interactions_train.csv\"\n",
    "data = NCFDataset(train_file=train_file, seed=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(item_min_timestamps.head())\n",
    "# print(news_df.columns)\n",
    "# print(news_df.filter(like=\"upload_time\").head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738526869.750389 1216378 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m NCF(\n\u001b[1;32m      3\u001b[0m     n_users\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mn_users,\n\u001b[1;32m      4\u001b[0m     n_items\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mn_items,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     seed\u001b[38;5;241m=\u001b[39mSEED\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Timer() \u001b[38;5;28;01mas\u001b[39;00m train_time:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining time:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_time)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Get top-k recommendations\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/recommenders/models/ncf/ncf_singlenode.py:392\u001b[0m, in \u001b[0;36mNCF.fit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    389\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# calculate loss and update NCF parameters\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser2id\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitem_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem2id\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/recommenders/models/ncf/dataset.py:527\u001b[0m, in \u001b[0;36mDataset.train_loader\u001b[0;34m(self, batch_size, shuffle_size, yield_id, write_to)\u001b[0m\n\u001b[1;32m    523\u001b[0m user_positive_item_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    524\u001b[0m     user_positive_examples[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_item]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m    525\u001b[0m )\n\u001b[1;32m    526\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_neg \u001b[38;5;241m*\u001b[39m user_positive_examples\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 527\u001b[0m sampler \u001b[38;5;241m=\u001b[39m \u001b[43mNegativeSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_positive_item_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_datafile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_with_replacement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_warnings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m user_negative_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_negative_examples_df(\n\u001b[1;32m    536\u001b[0m     user, sampler\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m    537\u001b[0m )\n\u001b[1;32m    538\u001b[0m user_examples \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[1;32m    539\u001b[0m     [user_positive_examples, user_negative_examples]\n\u001b[1;32m    540\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/recommenders/models/ncf/dataset.py:248\u001b[0m, in \u001b[0;36mNegativeSampler.__init__\u001b[0;34m(self, user, n_samples, user_positive_item_pool, item_pool, sample_with_replacement, print_warnings, training)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_warnings \u001b[38;5;241m=\u001b[39m print_warnings\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m training\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_negative_item_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_user_negatives_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopulation_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_negative_item_pool)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_negatives_with_replacement\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_with_replacement\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_negatives_without_replacement\n\u001b[1;32m    254\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/recommenders/models/ncf/dataset.py:267\u001b[0m, in \u001b[0;36mNegativeSampler._get_user_negatives_pool\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_user_negatives_pool\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# get list of items user has not interacted with\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_pool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_positive_item_pool\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train NCF model\n",
    "model = NCF(\n",
    "    n_users=data.n_users,\n",
    "    n_items=data.n_items,\n",
    "    model_type=\"NeuMF\",\n",
    "    n_factors=4,\n",
    "    layer_sizes=[16,8,4],\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=1e-3,\n",
    "    verbose=10,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "with Timer() as train_time:\n",
    "    model.fit(data)\n",
    "\n",
    "print(\"Training time:\", train_time)\n",
    "\n",
    "# Get top-k recommendations\n",
    "with Timer() as test_time:\n",
    "    users, items, preds = [], [], []\n",
    "    item = list(train_file[\"itemID\"].unique())\n",
    "    for user in train_file[\"userID\"].unique():\n",
    "        users.extend([user] * len(item))\n",
    "        items.extend(item)\n",
    "        preds.extend(model.predict([user] * len(item), item, is_list=True))\n",
    "\n",
    "    all_predictions = pd.DataFrame({\"userID\": users, \"itemID\": items, \"prediction\": preds})\n",
    "    all_predictions = all_predictions.sort_values(by=['userID', 'prediction'], ascending=[True, False])\n",
    "\n",
    "print(\"Prediction time:\", test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust batch size and learning rate\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Simplify model architecture\n",
    "model = NCF(\n",
    "    n_users=data.n_users,\n",
    "    n_items=data.n_items,\n",
    "    model_type=\"NeuMF\",\n",
    "    n_factors=4,\n",
    "    layer_sizes=[8, 4],  # Reduced layer sizes\n",
    "    n_epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    verbose=10,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Use early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Define early stopping parameters\n",
    "patience = 3  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')  # Track the best validation loss\n",
    "wait = 0  # Counter for epochs without improvement\n",
    "\n",
    "# Training loop with early stopping\n",
    "for epoch in range(EPOCHS):\n",
    "    # Train the model for one epoch\n",
    "    model.fit(data)  # Assuming model.fit() trains for one epoch\n",
    "    \n",
    "    # Evaluate the model on validation data\n",
    "    val_loss = evaluate_model(model, validation_data)  # You need to implement this function\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        wait = 0  # Reset the wait counter\n",
    "    else:\n",
    "        wait += 1  # Increment the wait counter\n",
    "        if wait >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break  # Stop training\n",
    "\n",
    "# with Timer() as train_time:\n",
    "#     model.fit(data, callbacks=[early_stopping])\n",
    "\n",
    "print(\"Training time:\", train_time)\n",
    "\n",
    "# Batch predictions for efficiency\n",
    "with Timer() as test_time:\n",
    "    users, items, preds = [], [], []\n",
    "    item = list(train_file[\"itemID\"].unique())\n",
    "    user_batch = train_file[\"userID\"].unique()\n",
    "    \n",
    "    # Predict in batches\n",
    "    for user in user_batch:\n",
    "        users.extend([user] * len(item))\n",
    "        items.extend(item)\n",
    "    \n",
    "    # Predict all at once\n",
    "    preds = model.predict(users, items, is_list=True)\n",
    "    \n",
    "    all_predictions = pd.DataFrame({\"userID\": users, \"itemID\": items, \"prediction\": preds})\n",
    "    all_predictions = all_predictions.sort_values(by=['userID', 'prediction'], ascending=[True, False])\n",
    "\n",
    "print(\"Prediction time:\", test_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate_at_k(ground_truth, predictions, k=TOP_K):\n",
    "    \"\"\"\n",
    "    Calculate Hit Rate at K (HR@K).\n",
    "\n",
    "    Args:\n",
    "        ground_truth (pd.DataFrame): Ground truth data with columns 'userID' and 'itemID'.\n",
    "        predictions (pd.DataFrame): Predicted recommendations with columns 'userID', 'itemID', and 'prediction'.\n",
    "        k (int): Number of top recommendations to consider.\n",
    "\n",
    "    Returns:\n",
    "        float: Hit Rate at K.\n",
    "    \"\"\"\n",
    "    # Group predictions by user and get top-K items for each user\n",
    "    top_k_predictions = predictions.groupby('userID').head(k)\n",
    "    \n",
    "    # Merge predictions with ground truth to find hits\n",
    "    hits = pd.merge(\n",
    "        top_k_predictions,\n",
    "        ground_truth,\n",
    "        on=['userID', 'itemID'],\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # Count the number of users with at least one hit\n",
    "    users_with_hits = hits['userID'].nunique()\n",
    "    total_users = ground_truth['userID'].nunique()\n",
    "    \n",
    "    # Calculate HR@K\n",
    "    hr_at_k = users_with_hits / total_users if total_users > 0 else 0\n",
    "    return hr_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "eval_ndcg = ndcg_at_k(train_file, all_predictions, col_prediction='prediction', k=TOP_K)\n",
    "eval_hr = hit_rate_at_k(train_file, all_predictions, k=TOP_K)  # Calculate HR@K\n",
    "\n",
    "# Coverage metrics\n",
    "exposed_new_items = all_predictions[all_predictions[\"itemID\"].isin(new_items)][\"itemID\"].nunique()\n",
    "total_new_items = len(new_items)\n",
    "new_item_coverage = exposed_new_items / total_new_items if total_new_items > 0 else 0\n",
    "\n",
    "total_items = all_predictions[\"itemID\"].nunique()\n",
    "overall_coverage = total_items / data.n_items\n",
    "\n",
    "# Print metrics\n",
    "print(\"NDCG@K:\", eval_ndcg)\n",
    "print(\"HR@K:\", eval_hr)  # Print HR@K\n",
    "print(\"New Item Coverage@K:\", new_item_coverage)\n",
    "print(\"Overall Item Coverage@K:\", overall_coverage)\n",
    "\n",
    "# Store results\n",
    "store_metadata(\"ndcg\", eval_ndcg)\n",
    "store_metadata(\"hr_at_k\", eval_hr)  # Store HR@K\n",
    "store_metadata(\"new_item_coverage\", new_item_coverage)\n",
    "store_metadata(\"overall_coverage\", overall_coverage)\n",
    "store_metadata(\"train_time\", train_time.interval)\n",
    "store_metadata(\"test_time\", test_time.interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
