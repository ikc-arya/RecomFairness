{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<i>Copyright (c) Recommenders contributors.</i>\n",
                "\n",
                "<i>Licensed under the MIT License.</i>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Neural Collaborative Filtering on MIND Dataset.\n",
                "\n",
                "Neural Collaborative Filtering (NCF) is a well known recommendation algorithm that generalizes the matrix factorization problem with multi-layer perceptron. \n",
                "\n",
                "This notebook provides an example of how to utilize and evaluate NCF implementation in the `recommenders`. We use a smaller dataset in this example to run NCF efficiently with GPU acceleration on a [Data Science Virtual Machine](https://azure.microsoft.com/en-gb/services/virtual-machines/data-science-virtual-machines/)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[autoreload of psutil failed: Traceback (most recent call last):\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
                        "    superreload(m, reload, self.old_objects)\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
                        "    module = reload(module)\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/importlib/__init__.py\", line 169, in reload\n",
                        "    _bootstrap._exec(spec, module)\n",
                        "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
                        "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
                        "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/psutil/__init__.py\", line 251, in <module>\n",
                        "    raise ImportError(msg)\n",
                        "ImportError: version conflict: '/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/psutil/_psutil_osx.cpython-39-darwin.so' C extension module was built for another version of psutil (5.8.0 instead of 6.1.1); you may try to 'pip uninstall psutil', manually remove /opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/psutil/_psutil_osx.cpython-39-darwin.so or clean the virtual env somehow, then reinstall\n",
                        "]\n",
                        "[autoreload of numpy failed: Traceback (most recent call last):\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
                        "    superreload(m, reload, self.old_objects)\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
                        "    module = reload(module)\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/importlib/__init__.py\", line 169, in reload\n",
                        "    _bootstrap._exec(spec, module)\n",
                        "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
                        "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
                        "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/numpy/__init__.py\", line 393, in <module>\n",
                        "    _mac_os_check()\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/numpy/__init__.py\", line 386, in _mac_os_check\n",
                        "    _ = polyfit(x, y, 2, cov=True)\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/numpy/lib/polynomial.py\", line 669, in polyfit\n",
                        "    c, resids, rank, s = lstsq(lhs, rhs, rcond)\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/numpy/linalg/linalg.py\", line 2326, in lstsq\n",
                        "    x, resids, rank, s = gufunc(a, b, rcond, signature=signature, extobj=extobj)\n",
                        "TypeError: lstsq_n() got an unexpected keyword argument 'extobj'\n",
                        "]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The autoreload extension is already loaded. To reload it, use:\n",
                        "  %reload_ext autoreload\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[autoreload of numpy.ma.core failed: Traceback (most recent call last):\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
                        "    superreload(m, reload, self.old_objects)\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
                        "    update_generic(old_obj, new_obj)\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
                        "    update(a, b)\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 335, in update_class\n",
                        "    if (old_obj == new_obj) is True:\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/numpy/ma/core.py\", line 4232, in __eq__\n",
                        "    \"\"\"\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/numpy/ma/core.py\", line 4160, in _comparison\n",
                        "    # Cast fill value to bool_ if needed. If it cannot be cast, the\n",
                        "  File \"/opt/anaconda3/envs/recommPaper/lib/python3.9/site-packages/numpy/ma/core.py\", line 1757, in mask_or\n",
                        "    return make_mask(umath.logical_or(m1, m2), copy=copy, shrink=shrink)\n",
                        "RecursionError: maximum recursion depth exceeded while calling a Python object\n",
                        "]\n"
                    ]
                }
            ],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "System version: 3.12.4 (main, Jun 16 2024, 01:58:47) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
                        "Pandas version: 2.2.3\n",
                        "Tensorflow version: 2.18.0\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import pandas as pd\n",
                "import tensorflow as tf\n",
                "tf.get_logger().setLevel('ERROR') # only show error messages\n",
                "\n",
                "from recommenders.utils.timer import Timer\n",
                "from recommenders.models.ncf.ncf_singlenode import NCF\n",
                "from recommenders.models.ncf.dataset import Dataset as NCFDataset\n",
                "from recommenders.datasets import movielens\n",
                "# from recommenders.datasets.python_splitters import python_chrono_split\n",
                "from recommenders.evaluation.python_evaluation import (\n",
                "    map, ndcg_at_k, precision_at_k, recall_at_k\n",
                ")\n",
                "from recommenders.utils.notebook_utils import store_metadata\n",
                "\n",
                "print(\"System version: {}\".format(sys.version))\n",
                "print(\"Pandas version: {}\".format(pd.__version__))\n",
                "print(\"Tensorflow version: {}\".format(tf.__version__))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Set the default parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model parameters\n",
                "EPOCHS = 50\n",
                "BATCH_SIZE = 256\n",
                "\n",
                "SEED = 42"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Load the MIND dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "behaviors_df = pd.read_csv(\"TaFR/data/MIND/MINDsmall_train/behaviors.tsv\", sep=\"\\t\", \n",
                "                           names=[\"impression_id\", \"user_id\", \"timestamp\", \"history\", \"impressions\"])\n",
                "\n",
                "# little preprocessing to get the interactions\n",
                "def process_impressions(row):\n",
                "    impressions = row[\"impressions\"].split(\" \")\n",
                "    return [(row[\"user_id\"], imp.split(\"-\")[0], int(imp.split(\"-\")[1]), row[\"timestamp\"]) \n",
                "            for imp in impressions]\n",
                "\n",
                "interactions = []\n",
                "for _, row in behaviors_df.iterrows():\n",
                "    interactions.extend(process_impressions(row))\n",
                "\n",
                "interactions_df = pd.DataFrame(interactions, columns=[\"userID\", \"itemID\", \"rating\", \"timestamp\"])\n",
                "\n",
                "news_df = pd.read_csv(\"TaFR/data/MIND/MINDsmall_train/news.tsv\", sep=\"\\t\", \n",
                "                      names=[\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"entity_list\", \"relation_list\"])\n",
                "\n",
                "interactions_df = interactions_df[interactions_df[\"itemID\"].isin(news_df[\"news_id\"])]\n",
                "\n",
                "# convert to csv\n",
                "interactions_df.to_csv(\"mind_interactions_train.csv\", index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Split the data using the Spark chronological splitter provided in utilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "## -*- not required in our dataset -*\n",
                "# train, test = python_chrono_split(df, 0.75)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Write datasets to csv files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_file = \"mind_interactions_train.csv\"\n",
                "# train.to_csv(train_file, index=False)\n",
                "# test_file = \"./test.csv\"\n",
                "# test.to_csv(test_file, index=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Generate an NCF dataset object from the data subsets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:recommenders.models.ncf.dataset:Indexing mind_interactions_train.csv ...\n"
                    ]
                },
                {
                    "ename": "ValueError",
                    "evalue": "invalid literal for int() with base 10: 'U13740'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mNCFDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/recommenders/models/ncf/dataset.py:354\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, train_file, test_file, test_file_full, overwrite_test_file_full, n_neg, n_neg_test, col_user, col_item, col_rating, binary, seed, sample_with_replacement, print_warnings)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_warnings \u001b[38;5;241m=\u001b[39m print_warnings\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_test_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_datafile \u001b[38;5;241m=\u001b[39m \u001b[43mDataFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_user\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol_user\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_item\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol_item\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcol_rating\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol_rating\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_users \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_datafile\u001b[38;5;241m.\u001b[39musers)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_datafile\u001b[38;5;241m.\u001b[39mitems)\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/recommenders/models/ncf/dataset.py:68\u001b[0m, in \u001b[0;36mDataFile.__init__\u001b[0;34m(self, filename, col_user, col_item, col_rating, col_test_batch, binary)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_fields\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_test_batch)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary \u001b[38;5;241m=\u001b[39m binary\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2user \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser2id[k]: k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser2id}\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid2item \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem2id[k]: k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem2id}\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/recommenders/models/ncf/dataset.py:150\u001b[0m, in \u001b[0;36mDataFile._init_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem2id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser2id \u001b[38;5;241m=\u001b[39m OrderedDict(), OrderedDict()\n\u001b[1;32m    149\u001b[0m batch_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 150\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol_item\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol_user\u001b[49m\u001b[43m]\u001b[49m\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/recommenders/models/ncf/dataset.py:105\u001b[0m, in \u001b[0;36mDataFile.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_row\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_row_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m EmptyFileException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is empty.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename))\n",
                        "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/recommenders/models/ncf/dataset.py:127\u001b[0m, in \u001b[0;36mDataFile._extract_row_data\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row\n\u001b[0;32m--> 127\u001b[0m user \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol_user\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_item])\n\u001b[1;32m    129\u001b[0m rating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(row[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_rating])\n",
                        "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'U13740'"
                    ]
                }
            ],
            "source": [
                "data = NCFDataset(train_file=train_file, seed=SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Train the NCF model on the training data, and get the top-k recommendations for our testing data\n",
                "\n",
                "NCF accepts implicit feedback and generates prospensity of items to be recommended to users in the scale of 0 to 1. A recommended item list can then be generated based on the scores. Note that this quickstart notebook is using a smaller number of epochs to reduce time for training. As a consequence, the model performance will be slighlty deteriorated. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = NCF (\n",
                "    n_users=data.n_users, \n",
                "    n_items=data.n_items,\n",
                "    model_type=\"NeuMF\",\n",
                "    n_factors=4,\n",
                "    layer_sizes=[16,8,4],\n",
                "    n_epochs=EPOCHS,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    learning_rate=1e-3,\n",
                "    verbose=10,\n",
                "    seed=SEED\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 10 [6.31s]: train_loss = 0.259318 \n",
                        "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 20 [6.28s]: train_loss = 0.246134 \n",
                        "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 30 [6.21s]: train_loss = 0.240125 \n",
                        "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 40 [6.23s]: train_loss = 0.235913 \n",
                        "INFO:recommenders.models.ncf.ncf_singlenode:Epoch 50 [6.31s]: train_loss = 0.232268 \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Took 317.7864 seconds for training.\n"
                    ]
                }
            ],
            "source": [
                "with Timer() as train_time:\n",
                "    model.fit(data)\n",
                "\n",
                "print(\"Took {} seconds for training.\".format(train_time))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In the movie recommendation use case scenario, seen movies are not recommended to the users."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Took 2.7835 seconds for prediction.\n"
                    ]
                }
            ],
            "source": [
                "with Timer() as test_time:\n",
                "    users, items, preds = [], [], []\n",
                "    item = list(train.itemID.unique())\n",
                "    for user in train.userID.unique():\n",
                "        user = [user] * len(item) \n",
                "        users.extend(user)\n",
                "        items.extend(item)\n",
                "        preds.extend(list(model.predict(user, item, is_list=True)))\n",
                "\n",
                "    all_predictions = pd.DataFrame(data={\"userID\": users, \"itemID\":items, \"prediction\":preds})\n",
                "\n",
                "    merged = pd.merge(train, all_predictions, on=[\"userID\", \"itemID\"], how=\"outer\")\n",
                "    all_predictions = merged[merged.rating.isnull()].drop('rating', axis=1)\n",
                "\n",
                "print(\"Took {} seconds for prediction.\".format(test_time))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Evaluate how well NCF performs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The ranking metrics are used for evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "MAP:\t0.049650\n",
                        "NDCG:\t0.200524\n",
                        "Precision@K:\t0.183033\n",
                        "Recall@K:\t0.102721\n"
                    ]
                }
            ],
            "source": [
                "eval_map = map(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
                "eval_ndcg = ndcg_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
                "eval_precision = precision_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
                "eval_recall = recall_at_k(test, all_predictions, col_prediction='prediction', k=TOP_K)\n",
                "\n",
                "print(\"MAP:\\t%f\" % eval_map,\n",
                "      \"NDCG:\\t%f\" % eval_ndcg,\n",
                "      \"Precision@K:\\t%f\" % eval_precision,\n",
                "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "NDCG - Normalised Discounted Cumulative Gain  \n",
                "MAP - Mean Average Precision"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Record results for tests - ignore this cell\n",
                "store_metadata(\"map\", eval_map)\n",
                "store_metadata(\"ndcg\", eval_ndcg)\n",
                "store_metadata(\"precision\", eval_precision)\n",
                "store_metadata(\"recall\", eval_recall)\n",
                "store_metadata(\"train_time\", train_time.interval)\n",
                "store_metadata(\"test_time\", test_time.interval)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "celltoolbar": "Tags",
        "kernelspec": {
            "display_name": "3.12.4",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
